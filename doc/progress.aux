\relax 
\providecommand\hyper@newdestlabel[2]{}
\bibstyle{unsrt}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{baudivs2011pachi}
\citation{coulom2006efficient}
\citation{silver2017masteringalphagozero}
\citation{gelly2007combining}
\citation{baudivs2011pachi}
\@writefile{toc}{\contentsline {section}{\numberline {1}Infrastructure}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Approach}{1}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Baseline}{1}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Oracle}{1}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Monte Carlo Tree Search}{1}{subsection.2.3}}
\newlabel{sec:mcts}{{2.3}{1}{Monte Carlo Tree Search}{subsection.2.3}{}}
\citation{williams1987reinforcement}
\citation{sutton2018reinforcement}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{algo:mcts}{{\caption@xref {algo:mcts}{ on input line 95}}{2}{Monte Carlo Tree Search}{algocf.caption.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces MCTS with UCB policy\relax }}{2}{algocf.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Self-play}{2}{subsection.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}REINFORCE with baseline}{2}{subsection.2.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}Network architecture}{2}{subsubsection.2.5.1}}
\newlabel{sec:nn}{{2.5.1}{2}{Network architecture}{subsubsection.2.5.1}{}}
\citation{silver2017masteringalphagozero}
\citation{sutton2018reinforcement}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}Training procedure}{3}{subsubsection.2.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Tree Search guided with NN}{3}{subsection.2.6}}
\citation{baudivs2011pachi}
\citation{gelly2007combining}
\citation{gelly2007combining}
\citation{bouzy2004monte}
\bibdata{reference}
\bibcite{baudivs2011pachi}{{1}{}{{}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experimental results \& error analysis}{4}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Monte Carlo Tree Search}{4}{subsection.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Results of 1000 games between MCTS and opponents on $9 \times 9$ board, no Komi, MCTS plays first\relax }}{4}{table.caption.3}}
\newlabel{tbl:mcts}{{1}{4}{Results of 1000 games between MCTS and opponents on $9 \times 9$ board, no Komi, MCTS plays first\relax }{table.caption.3}{}}
\bibcite{coulom2006efficient}{{2}{}{{}}{{}}}
\bibcite{silver2017masteringalphagozero}{{3}{}{{}}{{}}}
\bibcite{gelly2007combining}{{4}{}{{}}{{}}}
\bibcite{williams1987reinforcement}{{5}{}{{}}{{}}}
\bibcite{sutton2018reinforcement}{{6}{}{{}}{{}}}
\bibcite{bouzy2004monte}{{7}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Environment setup. For Go boards drawn in all the figures, 'x' represents black stone and 'o' represents white stone. For numpy arrays, '0' means empty, '1' represent black stone and '2' represents white stone. All examples assume $9 \times 9$ board.\relax }}{6}{figure.caption.4}}
\newlabel{fig:env}{{1}{6}{Environment setup. For Go boards drawn in all the figures, 'x' represents black stone and 'o' represents white stone. For numpy arrays, '0' means empty, '1' represent black stone and '2' represents white stone. All examples assume $9 \times 9$ board.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Appendix}{6}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Figures}{6}{subsection.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces MCTS node selection via tree policy: shows 4 successor states only for brevity. $a=67$ is selected. $n, q$ is visit count and value for each node respectively. $a=76$ has more visit counts but lower value.\relax }}{7}{figure.caption.5}}
\newlabel{fig:mctsselect}{{2}{7}{MCTS node selection via tree policy: shows 4 successor states only for brevity. $a=67$ is selected. $n, q$ is visit count and value for each node respectively. $a=76$ has more visit counts but lower value.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A new node $\text  {Succ}(s, a=7)$, randomly chosen, is expanded with zero visit count and value. \relax }}{7}{figure.caption.6}}
\newlabel{fig:mctsexpand}{{3}{7}{A new node $\text {Succ}(s, a=7)$, randomly chosen, is expanded with zero visit count and value. \relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces After the simulation is done. The result is backed up to all nodes on the path. Note the reward is from black player's perspective and needs to be adjusted for white player.\relax }}{8}{figure.caption.7}}
\newlabel{fig:mctsbackup}{{4}{8}{After the simulation is done. The result is backed up to all nodes on the path. Note the reward is from black player's perspective and needs to be adjusted for white player.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces After all rollout are done. Select the next move with maximum moves. Shows 3 successor states from root state only. The action $a=38$ is selected.\relax }}{9}{figure.caption.8}}
\newlabel{fig:mctsfinal}{{5}{9}{After all rollout are done. Select the next move with maximum moves. Shows 3 successor states from root state only. The action $a=38$ is selected.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces network architecture for policy gradients and NN guided MCTS approaches. The features and policy/value targets are gathered from self-play experiences. FC stands for Fully Connected layer. Features, targets and Loss are discussed in details in text.\relax }}{9}{figure.caption.9}}
\newlabel{fig:nn}{{6}{9}{network architecture for policy gradients and NN guided MCTS approaches. The features and policy/value targets are gathered from self-play experiences. FC stands for Fully Connected layer. Features, targets and Loss are discussed in details in text.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces example for an input/output for neural network. The input is $17 \times 9 \times 9 $ arrays, created from the board position. There are two outputs, one $82$-tuple of probability vector $\pi _{\theta }$ and one scalar $v_w \in [-1, 1]$. Refer to the text for more details. \relax }}{10}{figure.caption.10}}
\newlabel{fig:inference}{{7}{10}{example for an input/output for neural network. The input is $17 \times 9 \times 9 $ arrays, created from the board position. There are two outputs, one $82$-tuple of probability vector $\pi _{\theta }$ and one scalar $v_w \in [-1, 1]$. Refer to the text for more details. \relax }{figure.caption.10}{}}

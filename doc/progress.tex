\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2019}
\bibliographystyle{unsrt}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2019}

% to avoid loading the natbib package, add option nonatbibnatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{textcomp}
\usepackage{lmodern}% http://ctan.org/pkg/lm
\usepackage[table,x11names,svgnames]{xcolor}
\usepackage{soul}
\usepackage{parskip}
\usepackage{multirow}
\usepackage{array}
\usepackage{afterpage}
\usepackage{tabularx}
\usepackage{float}
\usepackage{placeins}
\usepackage{tablefootnote}
\usepackage{microtype}
\usepackage{textcomp}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{verbatim}
\usepackage[htt]{hyphenat}

\title{Learn to play Go}

\author{%
  Albert Liu \\
  Department of Computer Science\\
  Stanford University\\
  \texttt{albertpl@stanford.edu} \\
}

\begin{document}

\maketitle

\section{Infrastructure}
As illustrated in Figure \ref{fig:env}, our simulator includes a Go gameplay engine, which is based on Pachi \cite{baudivs2011pachi}, an open source Go framework. We choose Pachi due to its efficiency and simple APIs. Each agent is given a 2D numpy array (e.g. $19 \times 19$), representing current board with each element encoding the color of stone, 0 for empty, 1 for black stone and 2 for white stone. Then each agent returns the next move to play, encoded as position of the board (e.g. E4) and a pass action. 
Each agent can optionally persist the game records, numpy arrays that hold the board representation, moves and etc., to disk, which are used as experiences for the reinforcement learning agents. We use Keras as our deep learning framework. 

\section{Approach}
Minimax search or AlphaBeta search is intractable due to enormous search space. Instead,  we start with Monte Carlo Tree Search \cite{coulom2006efficient}. Then we explore policy gradient based methods. Finally we will combine tree search and function approximation approach, following AlphaGoZero \cite{silver2017masteringalphagozero}.  But first, we discuss the baseline and oracle.

\subsection{Baseline}
For baseline, we write a random agent which selects legal move randomly, except that it won't commit suicide, i.e. filling in its own eye, which is an empty position where all adjacent positions and three out of four diagonally adjacent positions are its stones (or edges).

\subsection{Oracle}
We use Pachi built-in \textit{UCT} engine as our oracle which is said to achieve highest amateur expect level (KGS 7 dan) on $9 \times 9$ or KGS 2d rank on reasonable hardware, where dan is expert Go ranking, from 1 to 7 (low to high). Technically the \textit{UCT} engine implements RAVE \cite{gelly2007combining}, instead of classic UCT. This engine incorporates lots of heuristics and predefined patterns above the basic Go rules, e.g. self-atari detector and ladder testing \cite{gelly2007combining}.

\subsection{Monte Carlo Tree Search}
The MCTS 
Each node in the game tree is a board representation and the player.

\subsection{Self-play}
In order to apply reinforcement learning, we need to collect experiences (i.e. game records). The standard approach is to have agents play against themselves and save the game records. We record the board position at each step perceived by the player, as a 2D matrix encoding player's color, the move of the player, the color of the player and the final reward (-1 if white player wins, +1 if black player wins).

Describe UCT algorithm. pro and con.
\subsection{Actor-critic with baseline}
The policy gradient based methods, such as REINFORCE [TODO] and actor-critic [TODO], allow us to learn stochastic policy naturally. For our episodic cases, we define the objective function, $J(\theta)$, to be the start value of any state $S$, following our parameterized policy function $\pi_{\theta}$ which is approximated by a deep neural network, i.e.,  $J(\theta) = V_{\pi_{\theta}}(S)$. By Policy Gradient Theorem [TODO], we can compute the gradient as
$$ \nabla_{\theta} J(\theta) =  \mathbb E_{\pi_{\theta}} [ 
Q_{\pi_{\theta}}(S_t, A_t)] 
\nabla_{\theta} \log \pi_{\theta}(A_t|S_t) 
$$

REINFORCE method uses Monte Carlo method to give unbiased sample $G_t$ for $Q_{\pi_{\theta}}(S_t, A_t)]$ and the update rule is
$$ \theta_{t+1} = \theta_{t} + \alpha 
G_t 
\nabla_{\theta} \log \pi_{\theta}(A_t|S_t) 
$$
But Monte Carlo gradient typically has high variance and produces slow learning. Actor-critic with baseline is suggested [TODO] to solve such high variance issue. Let advantage function $
A_{\pi_{\theta}}(S_t, A_t) 
= 
Q_{\pi_{\theta}}(S_t, A_t)  - 
V_{\pi_{\theta}}(S_t) 
$, the update rule is
$$ \theta_{t+1} = \theta_{t} + \alpha 
A_{\pi_{\theta}}(S_t, A_t) 
\nabla_{\theta} \log \pi_{\theta}(A_t|S_t) 
$$

\subsection{Tree Search guided with DNN}

\section{Error analysis}
Analyze why it is not able to beat Pachi. 



\section{Figures, tables, references}
\subsection{Figures}
\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{simulator}
\caption{Environment setup}
\label{fig:env}
\end{figure}

\bibliography{reference} 

\end{document}
